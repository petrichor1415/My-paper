# My-paper
## 医学图像
### 病理图像分割
**[ICLR 2023]  ITERATIVE PATCH SELECTION FOR HIGH-RESOLUTION IMAGE RECOGNITION**[[paper]](https://arxiv.org/pdf/2210.13007)[[code]](https://arxiv.org/pdf/2210.13007)

**[CVPR 2024] GraphCL: Graph-based Clustering for Semi-Supervised Medical Image Segmentation** [[paper]](https://arxiv.org/pdf/2411.13147v1) [[code]](https://github.com/ZiqinZhou66/ZegCLIP?tab=readme-ov-file)

**[2025] Prototype-Based Image Prompting for Weakly Supervised Histopathological Image Segmentation**[[paper]](https://arxiv.org/pdf/2503.12068)[[code]](https://github.com/QingchenTang/PBIP)

**[CVPR 2025] POT: Prototypical Optimal Transport for Weakly Supervised Semantic  Segmentation**[[paper]](https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_POT_Prototypical_Optimal_Transport_for_Weakly_Supervised_Semantic_Segmentation_CVPR_2025_paper.pdf)[[code]](https://github.com/jianwang91/POT)

**[CVPR 2025] Multi-Label Prototype Visual Spatial Search for Weakly Supervised Semantic Segmentation**[[paper]](https://openaccess.thecvf.com/content/CVPR2025/papers/Duan_Multi-Label_Prototype_Visual_Spatial_Search_for_Weakly_Supervised_Semantic_Segmentation_CVPR_2025_paper.pdf)[[code]]()

**[MICCAI 2025] D-CAM: Learning Generalizable Weakly-Supervised Medical Image Segmentation from Domain-invariant CAM**[[paper]](https://drive.google.com/file/d/1WHjQZ1i_npRdNV208jz0TGqlcY6pRdAP/view)[[code]](https://github.com/JingjunYi/D-CAM)

**[CVPR 2025] PH-Net: Semi-Supervised Breast Lesion Segmentation via Patch-wise Hardness**
[[Paper]](https://openaccess.thecvf.com/content/CVPR2024/papers/Jiang_PH-Net_Semi-Supervised_Breast_Lesion_Segmentation_via_Patch-wise_Hardness_CVPR_2024_paper.pdf)[[Code]](https://github.com/jjjsyyy/PH-Net)

**[MICCAI 2025] D-CAM: Learning Generalizable Weakly-Supervised Medical Image Segmentation from Domain-invariant CAM**[[paper]](https://drive.google.com/file/d/1P2J1OhXHcT1Uin8OVGgl6XL1BpmAS6bV/view?usp=drive_link)

**[MedIA 2025] UN-SAM: Domain-Adaptive Self-Prompt Segmentation for Universal Nuclei Images**[[paper]](https://www.sciencedirect.com/science/article/abs/pii/S1361841525001549)[[code]](https://github.com/CUHK-AIM-Group/UN-SAM)

**[2025] Test-time Adaptation for Foundation Medical Segmentation Model without Parametric Updates**[[paper]](https://www.arxiv.org/pdf/2504.02008)

**[MIDL 2025] Segment Anything for Histopathology**[[paper]](https://arxiv.org/pdf/2502.00408)[[code]](https://github.com/computational-cell-analytics/patho-sam)

**[MICCAI 2025] D-CAM: Learning Generalizable Weakly-Supervised Medical Image Segmentation from Domain-invariant CAM**[[paper]](https://papers.miccai.org/miccai-2025/paper/0830_paper.pdf)[[code]](https://github.com/JingjunYi/D-CAM)

**[MICCAI 2025] Semi-Supervised Multi-Modal Medical Image Segmentation for Complex Situations**[[paper]](https://papers.miccai.org/miccai-2025/paper/3439_paper.pdf)[[code]](https://github.com/DongdongMeng/SMMS)

**[CVPR 2025] Multi-modal Topology-embedded Graph Learning for Spatially Resolved Genes Prediction from Pathology Images with Prior Gene Similarity Information**[[paper]](https://openaccess.thecvf.com/content/CVPR2025/papers/Shi_Multi-modal_Topology-embedded_Graph_Learning_for_Spatially_Resolved_Genes_Prediction_from_CVPR_2025_paper.pdf)[[code]](https://github.com/shihangjs/M2TGLGO)

**[2025 1CLR] VLSA: lnterpretable Vision-Language Survival Analysis with Ordinal inductive Bias for Computational Pathology**[[paper]](https://arxiv.org/pdf/2409.09369)[[code]](https://github.com/liupei101/VLSA)

**[2025 Nature] A vision-language foundation model for precision oncology**[[paper]](https://www.nature.com/articles/s41586-024-08378-w)

**[2025 arXivl]ModalTune: Fine-Tuning Slide-Level Foundation Models with Multi-Modal Information for Multi-task Learning in Digital Pathology**[[paper]](https://arxiv.org/pdf/2503.17564)[[code]](https://github.com/martellab-sri/ModalTune)







**[ICLR 2026]SPROUT:TRAINING-FREENUCLEARINSTANCESEG MENTATIONWITHAUTOMATICPROMPTING**[[paper]](https://openreview.net/attachment?id=pqLlFR5ken&name=pdf))[[code]]()


### 病理图像分类
**[NeurIPS 2024] Boosting Vision-Language Models with Transduction** [[paper](https://proceedings.neurips.cc/paper_files/paper/2024/file/71d7dbe2652bd4662d29fa269f059db4-Paper-Conference.pdf)] [[code](https://github.com/MaxZanella/transduction-for-vlms)]  

**[MICCAI 2025] Hierarchical text-to-vision self supervised alignment for improved histopathology representation learning** [[paper](https://link.springer.com/chapter/10.1007/978-3-031-72083-3_16)] [[code](https://github.com/Hasindri/HLSS)]  

**[ECCV 2025] Knowledge-enhanced visual-language pretraining for computational pathology**[[paper](https://arxiv.org/pdf/2404.09942)] [[code](https://github.com/MAGIC-AI4Med/KEP)]  

**[LNCS 2024] Boosting Vision-Language Models for Histopathology Classification: Predict all at once**[[paper](https://arxiv.org/pdf/2409.01883)] [[code](https://github.com/FereshteShakeri/Histo-TransCLIP)]  

**[CVPR 2024] Generalizable whole slide image classification with fine-grained visual-semantic interaction**[[paper](https://openaccess.thecvf.com/content/CVPR2024/papers/Li_Generalizable_Whole_Slide_Image_Classification_with_Fine-Grained_Visual-Semantic_Interaction_CVPR_2024_paper.pdf)] [[code](https://github.com/ls1rius/WSI_FIVE)]  

**[CVPR 2024] ViLa-MIL: Dual-scale Vision-Language Multiple Instance Learning for Whole Slide Image Classification** [[paper](https://openaccess.thecvf.com/content/CVPR2024/papers/Shi_ViLa-MIL_Dual-scale_Vision-Language_Multiple_Instance_Learning_for_Whole_Slide_Image_CVPR_2024_paper.pdf)] [[code](https://github.com/Jiangbo-Shi/ViLa-MIL)]  

**[MICCAI 2024] Few-shot Adaptation of Medical Vision-Language Models**[[paper]](https://arxiv.org/pdf/2409.03868)[[code]](https://github.com/FereshteShakeri/few-shot-MedVLMs?tab=readme-ov-file)

**[CVPR 2025] No Pains, More Gains:Recycling Sub-Salient Patches for Efficient High-Resolution Image Recognition**[[paper]](https://openaccess.thecvf.com/content/CVPR2025/papers/Qin_No_Pains_More_Gains_Recycling_Sub-Salient_Patches_for_Efficient_High-Resolution_CVPR_2025_paper.pdf)[[code]](https://github.com/Qinrong-NKU/DBPS)

**[ 2025] THUNDER:Tile-level Histopathology image UNDERstanding benchmark**[[paper]](https://arxiv.org/pdf/2507.07860)[[code]](https://github.com/MICS-Lab/thunder)

**[2025] Uncertainty-Driven Expert Control: Enhancing the Reliability of Medical Vision-Language Models**[[paper]](https://www.arxiv.org/pdf/2507.09209)[[code]](https://github.com/ecoxial2007/Expert-CFG)

**[CVPR 2024] Transductive Zero-Shot and Few-Shot CLIP**[[paper]](https://openaccess.thecvf.com/content/CVPR2024/papers/Martin_Transductive_Zero-Shot_and_Few-Shot_CLIP_CVPR_2024_paper.pdf)[[code]](https://github.com/SegoleneMartin/transductive-CLIP)

**[IPMI 2025] A Reality Check of Vision-Language Pre-training in Radiology: Have We Progressed Using Text?**[[paper]](https://arxiv.org/pdf/2504.05227)[[code]](https://github.com/jusiro/DLILP)

**[MICCAI 2025] Few-Shot, Now for Real: Medical VLMs Adaptation without Balanced Sets or Validation**[[paper]](https://arxiv.org/pdf/2506.17500)[[code]](https://github.com/jusiro/SS-Text)

**[CVPR 2025] Fast and Accurate Gigapixel Pathological Image Classification with Hierarchical Distillation Multi-Instance Learning**[[paper](https://arxiv.org/pdf/2502.21130)] [[code](https://github.com/JiuyangDong/HDMIL)] 

**[CVPR 2025] MExD:AnExpert-InfusedDiffusionModelforWhole-SlideImageClassification**[[paper](https://arxiv.org/pdf/2503.12401)] [[code](https://github.com/JWZhao-uestc/MExD)]  

**[CVPR 2025] FOCUS: Knowledge-enhanced Adaptive Visual Compression for Few-shot Whole Slide Image Classification**[[paper](https://arxiv.org/pdf/2411.14743)] [[code](https://github.com/dddavid4real/FOCUS)]  

**[ICCV 2025] Rethinking Few-Shot Image Classification: a Good Embedding Is All You Need?**[[paper]](https://arxiv.org/pdf/2003.11539)[[code]](https://github.com/WangYueFt/rfs/)

**[ICLR 2025] Efficient and Context-Aware Label Propagation for Zero-/Few-Shot Training-Free Adaptation of Vision-Language Model**[[paper]](https://arxiv.org/pdf/2412.18303)[[code]](https://github.com/Yushu-Li/ECALP)

**[CVPR 2025] COSMIC:Clique-Oriented Semantic Multi-space Integration for Robust CLIP Test-Time Adaptation** [[paper](https://arxiv.org/pdf/2503.23388)] [[code](https://github.com/hf618/COSMIC)]  

**[CVPR 2025] Text Augmented Correlation Transformer For Few-shot Classification & Segmentation**[[paper]](https://openaccess.thecvf.com/content/CVPR2025/papers/Nandam_Text_Augmented_Correlation_Transformer_For_Few-shot_Classification__Segmentation_CVPR_2025_paper.pdf)

**[ICCV 2025] Think Twice: Test-Time Reasoning for Robust CLIP Zero-Shot Classification**[[paper]](https://iccv.thecvf.com/virtual/2025/poster/1492)

**[ICCV 2025] One Last Attention for your Vision-Language Model**[[paper]](https://arxiv.org/pdf/2507.15480)[[code]](https://github.com/khufia/RAda/tree/main)

**[MICCAI 2025] Conservative-Radical Complementary Learning for Class-incremental Medical Image Analysis with Pre-trained Foundation Models**[[paper]](https://papers.miccai.org/miccai-2025/paper/0019_paper.pdf)[[code]](https://github.com/CUHK-BMEAI/CRCL)





**[NeurIPS 2024]FAST: A Dual-tier Few-Shot Learning Paradigm for Whole Slide Image Classification**[[paper]](https://proceedings.neurips.cc/paper_files/paper/2024/file/bdcdf38389d7fcefc73c4c3720217155-Paper-Conference.pdf)[[code]](https://github.com/fukexue/FAST)

**[ICLR 2026] FLOW-BASED ALIGNMENT OF UNI-MODAL VISION AND TEXT ENCODERS FOR FEW-SHOT IMAGE CLASSIFICATION**[[paper]](https://openreview.net/attachment?id=educGk5ykl&name=pdf)[[code]]()

**[ICLR 2026]  EXPLOITING LOW-DIMENSIONAL MANIFOLD OF FEATURES FORFEW-SHOT WHOLE SLIDE IMAGE CLASSIFICATION**[[paper]](https://openreview.net/attachment?id=HBP9uSEYME&name=pdf)[[code]]()


### 病理图像检测
**[CVPR 2024] Adapting Visual-Language Models for Generalizable Anomaly Detection in Medical Images**
[[Paper]](https://openaccess.thecvf.com/content/CVPR2024/papers/Huang_Adapting_Visual-Language_Models_for_Generalizable_Anomaly_Detection_in_Medical_Images_CVPR_2024_paper.pdf)
[[Code]](https://github.com/MediaBrain-SJTU/MVFA-AD)

**[ICCV 2025] Unknown Text Learning for CLIP-based Few-Shot Open-set Recognition**[[paper]](https://iccv.thecvf.com/virtual/2025/poster/2460)

### Foundation Models & Pre-training
**[2023-Arxiv] Foundation Models in Computational Pathology: A Review of Challenges, Opportunities, and Impact**[[paper](https://arxiv.org/pdf/2502.08333)]  

**[CVPR 2025] CPath-Omni: A Unified Multimodal Foundation Model for Patch and Whole Slide Image Analysis in Computational Pathology**[[paper](https://arxiv.org/pdf/2412.12077)]  

**[CVPR 2025] ulti-Resolution Pathology-Language Pre-training Model with Text-Guided Visual Representation**[[paper](https://openaccess.thecvf.com/content/CVPR2025/papers/Albastaki_Multi-Resolution_Pathology-Language_Pre-training_Model_with_Text-Guided_Visual_Representation_CVPR_2025_paper.pdf)] [[code](https://github.com/BasitAlawode/MR-PLIP)]  

**[Nature 2025] BEPH:A foundation model for generalizable cancer diagnosis and survival prediction from histopathological images**[[paper](https://www.nature.com/articles/s41467-025-57587-y#Sec10)] [[code](https://github.com/Zhcyoung/BEPH)]  

**[Nature 2025] A vision–language foundation model for precision oncology**[[paper](https://www.nature.com/articles/s41586-024-08378-w#code-availability)]  

**[CVPR 2025] Realistic Test-Time Adaptation of Vision-Language Models** [[paper](https://openaccess.thecvf.com/content/CVPR2025/papers/Zanella_Realistic_Test-Time_Adaptation_of_Vision-Language_Models_CVPR_2025_paper.pdf)] [[code](https://github.com/MaxZanella/StatA)]  

**[2025-Arxiv] GECKO:GigapixelVision-Concept Contrastive Pretraining in Histopathology** [[paper](https://arxiv.org/pdf/2504.01009)] [[code](https://github.com/bmi-imaginelab/GECKO)]  

**[2025-Arxiv] Multi-Modal Foundation Models for Computational Pathology:A Survey**[[paper](https://arxiv.org/pdf/2503.09091)]  

**[2025-Arxiv] Unsupervised Foundation Model-Agnostic Slide-Level Representation Learning** [[paper](https://arxiv.org/pdf/2411.13623)] [[code](https://github.com/KatherLab/COBRA)] 

**[2025-Arxiv] Full Conformal Adaptation of Medical Vision-Language Models** [[paper](https://arxiv.org/pdf/2506.06076)] [[code](https://github.com/jusiro/FCA)]  

**[CVPR 2025] GET: Unlocking the Multi-modal Potential of CLIP for Generalized Category Discovery**[[paper](https://arxiv.org/pdf/2403.09974)] [[code](https://github.com/enguangW/GET)]  

**[ICCV 2025] One Last Attention for your Vision-Language Model**[[paper]](https://github.com/khufia/RAda/tree/main)[[code]](https://github.com/khufia/RAda/tree/main)

**[ICCV 2025] CLIP-GS: Unifying Vision-Language Representation with 3D Gaussian Splatting**[[paper]](https://arxiv.org/pdf/2412.19142)[[code]]()

**[ICCV 2025] Multi-Cache Enhanced Prototype Learning for Test-Time Generalization of Vision-Language Models**[[paper]](https://iccv.thecvf.com/virtual/2025/poster/1177)

**[ICCV 2025] Test-Time Retrieval-Augmented Adaptation for Vision-Language Models**[[paper]](https://iccv.thecvf.com/virtual/2025/poster/2327)


### Few-shot合集
**[ICCV 2023] Black Box Few-Shot Adaptation for Vision-Language models**[[paper]](https://openaccess.thecvf.com/content/ICCV2023/papers/Ouali_Black_Box_Few-Shot_Adaptation_for_Vision-Language_Models_ICCV_2023_paper.pdf)[[code]](https://github.com/saic-fi/LFA)

**[2024]Selective Vision-Language Subspace Projection for Few-shot CLIP**[[paper]](https://arxiv.org/pdf/2407.16977?)[[code]](https://github.com/zhuhsingyuu/SSP)

**[ICCV 2025] CorrCLIP: Reconstructing Patch Correlations in CLIP for Open-Vocabulary Semantic Segmentation**[[paper]](https://arxiv.org/pdf/2411.10086)[[code]](https://github.com/zdk258/CorrCLIP)


## 综述
**[2025] Adapting Vision-Language Models Without Labels:A Comprehensive Survey**[[paper]](https://arxiv.org/pdf/2508.05547?)[[code]](https://github.com/tim-learn/Awesome-LabelFree-VLMs)


## KNN & retrieval

**1、NeurIPS 2021]Re-ranking for image retrieval and transductive few-shot classification**[[paper]](https://proceedings.neurips.cc/paper_files/paper/2021/file/d9fc0cdb67638d50f411432d0d41d0ba-Paper.pdf)[[code]](https://imagine.enpc.fr/~shenx/SSR/)

**2、[CVPR 2017]Re-ranking Person Re-identification with k-reciprocal Encoding**[[paper]](https://arxiv.org/pdf/1701.08398)[[code]](https://github.com/zhunzhong07/person-re-ranking)

**3、[CVPR 2025]Cheb-GR: Rethinking k-nearest neighbor search in Re-ranking for Person Re-identification**[[paper]](https://openaccess.thecvf.com/content/CVPR2025/papers/Yang_Cheb-GR_Rethinking_K-nearest_Neighbor_Search_in_Re-ranking_for_Person_Re-identification_CVPR_2025_paper.pdf)[[code]](https://github.com/Jinxi-Yang-WHU/Fast-GCR)






## 自然图像
### Weakly Supervised Semantic Segmentation

 **1、Learning Multi-Modal Class-Specific Tokens for Weakly Supervised Dense Object Localization**[[paper]](https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Learning_Multi-Modal_Class-Specific_Tokens_for_Weakly_Supervised_Dense_Object_Localization_CVPR_2023_paper.pdf)[[code]](https://github.com/xulianuwa/MMCST)

**2、[CVPR 2022]Learning Affinity from Attention: End-to-End Weakly-Supervised Semantic Segmentation with Transformers**[[paper]](https://openaccess.thecvf.com/content/CVPR2022/papers/Ru_Learning_Affinity_From_Attention_End-to-End_Weakly-Supervised_Semantic_Segmentation_With_Transformers_CVPR_2022_paper.pdf)[[code]](https://github.com/rulixiang/afa)

**3、[CVPR 2022]MCTFormer:Multi-Class Token Transformer for Weakly Supervised Semantic Segmentation**[[paper]](https://arxiv.org/pdf/2203.02891)[[code]](https://github.com/xulianuwa/MCTformer)

**4、[CVPR 2022] MCTformer+: Multi-Class Token Transformer for Weakly Supervised Semantic Segmentation**[[paper]](https://arxiv.org/pdf/2308.03005)[[code]](https://github.com/xulianuwa/MCTformer)

**5、[ICCV 2023]Spatial-Aware Token for Weakly Supervised Object Localization** [[paper]](https://openaccess.thecvf.com/content/ICCV2023/papers/Wu_Spatial-Aware_Token_for_Weakly_Supervised_Object_Localization_ICCV_2023_paper.pdf) [[code](https://github.com/wpy1999/SAT))

**6、[CVPR 2023]Boundary-enhanced Co-training for Weakly Supervised Semantic Segmentation**[[paper]](https://openaccess.thecvf.com/content/CVPR2023/papers/Rong_Boundary-Enhanced_Co-Training_for_Weakly_Supervised_Semantic_Segmentation_CVPR_2023_paper.pdf)[[code]](https://github.com/ShenghaiRong/BECO?tab=readme-ov-file)

**7、[ICCV 2023]Zero-Shot Composed Image Retrieval with Textual Inversion**[[paper]](https://openaccess.thecvf.com/content/ICCV2023/papers/Baldrati_Zero-Shot_Composed_Image_Retrieval_with_Textual_Inversion_ICCV_2023_paper.pdf)[[code]](https://github.com/miccunifi/SEARLE.)

**8、[CVPR 2023]Learning Multi-Modal Class-Specific Tokens for Weakly Supervised Dense Object Localization** [[paper]](https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Learning_Multi-Modal_Class-Specific_Tokens_for_Weakly_Supervised_Dense_Object_Localization_CVPR_2023_paper.pdf) [[code]](https://github.com/xulianuwa/MMCST)\

**9、[CVPR 2023]ToCo:Token Contrast for Weakly-Supervised Semantic Segmentation**[[paper]](https://arxiv.org/pdf/2303.01267)[[code]](https://github.com/rulixiang/ToCo)

**10、[CVPR 2024]Frozen CLIP: A Strong Backbone for Weakly Supervised Semantic Segmentation** [[paper]](https://arxiv.org/pdf/2406.11189v1) [[code]](https://github.com/zbf1991/WeCLIP)

**11、[CVPR 2024]DuPL: Dual Student with Trustworthy Progressive Learning for RobustWeakly Supervised Semantic Segmentation** [[paper]](https://arxiv.org/pdf/2403.11184)[[code]](https://github.com/Wu0409/DuPL)

**12、[CVPR 2024]Hunting Attributes: Context Prototype-Aware Learning for Weakly Supervised Semantic Segmentation** [[paper]](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2403.07630)[[code]](https://github.com/Barrett-python/CPAL))

**13、[ECCV 2024]DIAL: Dense Image-text ALignment for Weakly Supervised Semantic Segmentation**[[paper]](https://arxiv.org/pdf/2409.15801)

**14、[CVPR 2024]Separate and Conquer: Decoupling Co-occurrence via Decomposition and Representation for Weakly Supervised Semantic Segmentation**[[paper]](https://arxiv.org/pdf/2402.18467)[[code]](https://github.com/zwyang6/SeCo)

**15、[ECCV 2024]CoSa:Weakly Supervised Co-training with Swapping Assignments for Semantic Segmentation**[[paper]](https://arxiv.org/pdf/2402.17891)[[code]](https://github.com/youshyee/CoSA)

**16、[IEEE 2024]SSC:Spatial Structure Constraints for Weakly Supervised Semantic Segmentation**[[paper]](https://arxiv.org/pdf/2401.11122)[[code](https://github.com/NUST-Machine-Intelligence-Laboratory/SSC)

**17、[AAAI 2024]Progressive Feature Self-Reinforcement for Weakly Supervised Semantic Segmentation**[[paper]](https://arxiv.org/pdf/2312.08916)[[code]](https://github.com/Jessie459/feature-self-reinforcement)

**18、[CVPR 2024]Class Tokens Infusion for Weakly Supervised Semantic Segmentation**[[paper]](https://openaccess.thecvf.com/content/CVPR2024/papers/Yoon_Class_Tokens_Infusion_for_Weakly_Supervised_Semantic_Segmentation_CVPR_2024_paper.pdf)[[code]]( https://github.com/yoon307/CTI)

**19、[CVPR 2024]SFC: Shared Feature Calibration in Weakly Supervised Semantic Segmentation**[[paper]](https://arxiv.org/pdf/2401.11719)[[code]](https://github.com/Barrett-python/SFC)

**20、[CVPR 2024]PSDPM:Prototype-based Secondary Discriminative Pixels Mining for Weakly Supervised Semantic Segmentation**[[paper]](https://openaccess.thecvf.com/content/CVPR2024/papers/Zhao_PSDPM_Prototype-based_Secondary_Discriminative_Pixels_Mining_for_Weakly_Supervised_Semantic_CVPR_2024_paper.pdf)[[code]](https://github.com/xinqiaozhao/PSDPM)

**21、[AAAI 2025]MoRe: Class Patch Attention Needs Regularization for Weakly Supervised Semantic Segmentation**[[paper]](https://arxiv.org/pdf/2412.11076)[[code]](https://github.com/zwyang6/MoRe)

**22、[CVPR 2025]PROMPT-CAM: A Simpler Interpretable Transformer for Fine-Grained Analysis**[[paper]](https://arxiv.org/pdf/2501.09333)[[code]](https://github.com/Imageomics/Prompt_CAM)

**23、[CVPR 2025]Exploring CLIP’s Dense Knowledge for Weakly、 Supervised Semantic Segmentation**[[paper]](https://arxiv.org/pdf/2503.20826)[[code]](https://github.com/zwyang6/ExCEL)

**24、[CVPR 2025]GET: Unlocking the Multi-modal Potential of CLIP for Generalized Category Discovery**[[paper]](https://arxiv.org/abs/2403.09974)[[code]](https://github.com/enguangW/GET)

**25、[2025]TeD-Loc: Text Distillation for Weakly Supervised Object Localization**[[paper]](https://arxiv.org/pdf/2501.12632)[[code]](https://github.com/shakeebmurtaza/TeDLOC)

**26、[2025]Image Augmentation Agent for Weakly Supervised Semantic Segmentation**[[paper]](https://arxiv.org/pdf/2412.20439)

**27、[CVPR 2025]Multi-Label Prototype Visual Spatial Search for Weakly Supervised Semantic Segmentation**

### 分类
**[ICCV 2025] Generate, Transduct, Adapt: Iterative Transduction with VLMs**[[paper]](https://arxiv.org/pdf/2501.06031)[[code]](https://github.com/cvl-umass/GTA-CLIP))


### Weakly Supervised Object Localization
**1、[2024]A Realistic Protocol for Evaluation of Weakly Supervised Object Localization**[[paper](https://arxiv.org/pdf/2404.10034)[[code]](https://github.com/shakeebmurtaza/wsol_model_selection)

## 聚类
**1、[ICML 2024]Image Clustering with External Guidance**[[paper]](https://arxiv.org/pdf/2310.11989)[[code]](https://github.com/XLearning-SCU/2024-ICML-TAC)

**2、[TPAMI 2024]Simplex Clustering via sBeta With Applications to Online Adjustment of Black-Box Predictions**[[paper]](https://arxiv.org/pdf/2208.00287)[[code]](https://github.com/fchiaroni/Clustering_Softmax_Predictions)

## BackBone
**1、[CVPR 2016] CAM:Learning Deep Features for Discriminative Localization**
[[paper]](https://arxiv.org/pdf/1512.04150)

**2、[ICCV 2021] Swin Transformer: Hierarchical Vision Transformer using Shifted Windows**
[[paper]](https://arxiv.org/pdf/2103.14030)[[code]](https://github.com/microsoft/Swin-Transformer)

## benchmark
https://github.com/FereshteShakeri/few-shot-MedVLMs

**1、[NeurIPS25 D&B Spotlight] A tile-level histopathology image understanding benchmark**[[paper]](https://arxiv.org/pdf/2507.07860)[[code]](https://github.com/MICS-Lab/thunder)



<a name="Dataset"></a>
# Dataset:
- [x] [Cityscapes](https://github.com/open-mmlab/mmsegmentation/blob/master/docs/en/dataset_prepare.md#cityscapes)
- [x] [PASCAL VOC](https://github.com/open-mmlab/mmsegmentation/blob/master/docs/en/dataset_prepare.md#pascal-voc)
- [x] [ADE20K](https://github.com/open-mmlab/mmsegmentation/blob/master/docs/en/dataset_prepare.md#ade20k)
- [x] [Pascal Context](https://github.com/open-mmlab/mmsegmentation/blob/master/docs/en/dataset_prepare.md#pascal-context)
- [x] [COCO-Stuff 10k](https://github.com/open-mmlab/mmsegmentation/blob/master/docs/en/dataset_prepare.md#coco-stuff-10k)
- [x] [COCO-Stuff 164k](https://github.com/open-mmlab/mmsegmentation/blob/master/docs/en/dataset_prepare.md#coco-stuff-164k)
- [x] [CHASE_DB1](https://github.com/open-mmlab/mmsegmentation/blob/master/docs/en/dataset_prepare.md#chase-db1)
- [x] [DRIVE](https://github.com/open-mmlab/mmsegmentation/blob/master/docs/en/dataset_prepare.md#drive)
- [x] [HRF](https://github.com/open-mmlab/mmsegmentation/blob/master/docs/en/dataset_prepare.md#hrf)
- [x] [STARE](https://github.com/open-mmlab/mmsegmentation/blob/master/docs/en/dataset_prepare.md#stare)
- [x] [Dark Zurich](https://github.com/open-mmlab/mmsegmentation/blob/master/docs/en/dataset_prepare.md#dark-zurich)
- [x] [Nighttime Driving](https://github.com/open-mmlab/mmsegmentation/blob/master/docs/en/dataset_prepare.md#nighttime-driving)
- [x] [LoveDA](https://github.com/open-mmlab/mmsegmentation/blob/master/docs/en/dataset_prepare.md#loveda)
- [x] [Potsdam](https://github.com/open-mmlab/mmsegmentation/blob/master/docs/en/dataset_prepare.md#isprs-potsdam)
- [x] [Vaihingen](https://github.com/open-mmlab/mmsegmentation/blob/master/docs/en/dataset_prepare.md#isprs-vaihingen)
- [x] [iSAID](https://github.com/open-mmlab/mmsegmentation/blob/master/docs/en/dataset_prepare.md#isaid)
- [x] [High quality synthetic face occlusion](https://github.com/open-mmlab/mmsegmentation/blob/master/docs/en/dataset_prepare.md#delving-into-high-quality-synthetic-face-occlusion-segmentation-datasets)
- [x] [ImageNetS](https://github.com/open-mmlab/mmsegmentation/blob/master/docs/en/dataset_prepare.md#imagenets)

