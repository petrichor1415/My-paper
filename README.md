# My-paper
## 分类
**[ICML 2021]CLIP：Learning Transferable Visual Models From Natural Language Supervision**
- Paper:https://arxiv.org/pdf/2103.00020
- Code:https://github.com/OpenAI/CLIP
- Innovate:Text-image contrast and training.


## 分割
### Open-Vocabulary Semantic Segmentation
**[2022 ECCV]Mask CLIP:Extract Free Dense Labels from CLIP**
- Paper:https://arxiv.org/pdf/2112.01071
- Code:https://github.com/chongzhou96/MaskCLIP
- Innovate:The MaskCLIP and MaskCLIP + models are proposed.

**[2022 ICLR]LANGUAGE-DRIVEN SEMANTIC SEGMENTATION**
- Paper:https://arxiv.org/pdf/2201.03546
- Code:https://github.com/isl-org/lang-seg
- Innovate:Image encoder: based on dense prediction transformer (DPT).
- 

**[2024 ICLR] CLIPSelf: Vision Transformer Distills Itself for Open-Vocabulary Dense Prediction**
- Paper:https://arxiv.org/abs/2310.01403
- Code:https://github.com/wusize/CLIPSelf
- Innovate:No text is required, and the image is cropped to align with the full image.


**[2024 CVPR] SED: A Simple Encoder-Decoder for Open-Vocabulary Semantic Segmentation**
- Paper:https://arxiv.org/abs/2311.15537
- Code:https://github.com/xb534/SED.git.
- Innovate:A hierarchical encoder and progressive fusion feature are used, as well as early category rejection.

**[2024 ECCV] CLIP-DINOiser: Teaching CLIP a few DINO tricks for open-vocabulary semantic segmentation**
- Paper:https://arxiv.org/pdf/2312.12359
- Code:https://github.com/wysoczanska/clip_dinoiser
- Innovate:The positioning prior information of DINO is introduced to guide the pooling of MaskCLIP and features.

**[2024 ECCV] ProxyCLIP: Proxy Attention Improves CLIP for Open-Vocabulary Segmentation**
- Paper:https://arxiv.org/pdf/2408.04883
- Code:https://github.com/mc-lan/ProxyCLIP
- Innovate:

**[2024 ECCV] Pay Attention to Your Neighbours: Training-Free Open-Vocabulary Semantic Segmentation**
- Paper:https://arxiv.org/pdf/2404.08181
- Code:https://github.com/sinahmr/NACLIP
- Innovate:

**[2024 ECCV] ClearCLIP: Decomposing CLIP Representations for Dense Vision-Language Inference**
- Paper:https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/06346.pdf
- Code:https://github.com/mc-lan/ClearCLIP
- Innovate:

**[2024 ECCV] SCLIP: Rethinking Self-Attention for Dense Vision-Language Inference**
- Paper:https://arxiv.org/pdf/2312.01597
- Code:https://github.com/wangf3014/SCLIP
- Innovate:


## 目标检测（Object Detection）
**[CVPR 2025]LLMDet: Learning Strong Open-Vocabulary Object Detectors under the Supervision of Large Language Models**
- Paper:https://arxiv.org/pdf/2501.18954
- Code:https://github.com/iSEE-Laboratory/LLMDet
- Innovate:LLM generates image level and region level descriptions.

##  (数据集)Dataset

