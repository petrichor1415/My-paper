# My-paper
## 医学图像
### 病理图像分割
**[ICLR 2023]  ITERATIVE PATCH SELECTION FOR HIGH-RESOLUTION IMAGE RECOGNITION**[[paper]](https://arxiv.org/pdf/2210.13007)[[code]](https://arxiv.org/pdf/2210.13007)

**[CVPR 2024] GraphCL: Graph-based Clustering for Semi-Supervised Medical Image Segmentation** [[paper]](https://arxiv.org/pdf/2411.13147v1) [[code]](https://github.com/ZiqinZhou66/ZegCLIP?tab=readme-ov-file)

**[2025] Prototype-Based Image Prompting for Weakly Supervised Histopathological Image Segmentation**[[paper]](https://arxiv.org/pdf/2503.12068)[[code]](https://github.com/QingchenTang/PBIP)

**[CVPR 2025] POT: Prototypical Optimal Transport for Weakly Supervised Semantic  Segmentation**[[paper]](https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_POT_Prototypical_Optimal_Transport_for_Weakly_Supervised_Semantic_Segmentation_CVPR_2025_paper.pdf)[[code]](https://github.com/jianwang91/POT)

**[CVPR 2025] Multi-Label Prototype Visual Spatial Search for Weakly Supervised Semantic Segmentation**[[paper]](https://openaccess.thecvf.com/content/CVPR2025/papers/Duan_Multi-Label_Prototype_Visual_Spatial_Search_for_Weakly_Supervised_Semantic_Segmentation_CVPR_2025_paper.pdf)[[code]]()

**[MICCAI 2025] D-CAM: Learning Generalizable Weakly-Supervised Medical Image Segmentation from Domain-invariant CAM**[[paper]](https://drive.google.com/file/d/1WHjQZ1i_npRdNV208jz0TGqlcY6pRdAP/view)[[code]](https://github.com/JingjunYi/D-CAM)

**[CVPR 2025] PH-Net: Semi-Supervised Breast Lesion Segmentation via Patch-wise Hardness**
[[Paper]](https://openaccess.thecvf.com/content/CVPR2024/papers/Jiang_PH-Net_Semi-Supervised_Breast_Lesion_Segmentation_via_Patch-wise_Hardness_CVPR_2024_paper.pdf)[[Code]](https://github.com/jjjsyyy/PH-Net)

**[MICCAI 2025] D-CAM: Learning Generalizable Weakly-Supervised Medical Image Segmentation from Domain-invariant CAM**[[paper]](https://drive.google.com/file/d/1P2J1OhXHcT1Uin8OVGgl6XL1BpmAS6bV/view?usp=drive_link)

**[MedIA 2025] UN-SAM: Domain-Adaptive Self-Prompt Segmentation for Universal Nuclei Images**[[paper]](https://www.sciencedirect.com/science/article/abs/pii/S1361841525001549)[[code]](https://github.com/CUHK-AIM-Group/UN-SAM)

**[2025] Test-time Adaptation for Foundation Medical Segmentation Model without Parametric Updates**[[paper]](https://www.arxiv.org/pdf/2504.02008)

**[MIDL 2025] Segment Anything for Histopathology**[[paper]](https://arxiv.org/pdf/2502.00408)[[code]](https://github.com/computational-cell-analytics/patho-sam)

**[MICCAI 2025] D-CAM: Learning Generalizable Weakly-Supervised Medical Image Segmentation from Domain-invariant CAM**[[paper]](https://papers.miccai.org/miccai-2025/paper/0830_paper.pdf)[[code]](https://github.com/JingjunYi/D-CAM)

**[MICCAI 2025] Semi-Supervised Multi-Modal Medical Image Segmentation for Complex Situations**[[paper]](https://papers.miccai.org/miccai-2025/paper/3439_paper.pdf)[[code]](https://github.com/DongdongMeng/SMMS)

**[CVPR 2025] Multi-modal Topology-embedded Graph Learning for Spatially Resolved Genes Prediction from Pathology Images with Prior Gene Similarity Information**[[paper]](https://openaccess.thecvf.com/content/CVPR2025/papers/Shi_Multi-modal_Topology-embedded_Graph_Learning_for_Spatially_Resolved_Genes_Prediction_from_CVPR_2025_paper.pdf)[[code]](https://github.com/shihangjs/M2TGLGO)

**[2025 1CLR] VLSA: lnterpretable Vision-Language Survival Analysis with Ordinal inductive Bias for Computational Pathology**[[paper]](https://arxiv.org/pdf/2409.09369)[[code]](https://github.com/liupei101/VLSA)

**[2025 Nature] A vision-language foundation model for precision oncology**[[paper]](https://www.nature.com/articles/s41586-024-08378-w)

**[2025 arXivl]ModalTune: Fine-Tuning Slide-Level Foundation Models with Multi-Modal Information for Multi-task Learning in Digital Pathology**[[paper]](https://arxiv.org/pdf/2503.17564)[[code]](https://github.com/martellab-sri/ModalTune)

**[2025 NeurIPS]Segment Anything Model Meets Semi-supervised Medical Image Segmentation: A Novel Perspective**[[paper]](https://openreview.net/pdf?id=wHx7UuRm7G)[[code]]()

**[NeurIPS 2025] Revisiting End-to-End Learning with Slide-level Supervision in Computational Pathology**[[paper]](https://arxiv.org/pdf/2506.02408)[[code]](https://github.com/DearCaat/E2E-WSI-ABMILX)

**[ICLR 2026]SPROUT:TRAINING-FREENUCLEARINSTANCESEG MENTATIONWITHAUTOMATICPROMPTING**[[paper]](https://openreview.net/attachment?id=pqLlFR5ken&name=pdf))[[code]]()

**[NeurIPS 2025] Cancer Survival Analysis via Zero-shot Tumor Microenvironment Segmentation on Low-resolution Whole Slide Pathology Images**[[paper]](https://openreview.net/pdf?id=BkSRQ1y37l)[[code]]()


### 病理图像分类
**[NeurIPS 2024] Boosting Vision-Language Models with Transduction** [[paper](https://proceedings.neurips.cc/paper_files/paper/2024/file/71d7dbe2652bd4662d29fa269f059db4-Paper-Conference.pdf)] [[code](https://github.com/MaxZanella/transduction-for-vlms)]  

**[MICCAI 2025] Hierarchical text-to-vision self supervised alignment for improved histopathology representation learning** [[paper](https://link.springer.com/chapter/10.1007/978-3-031-72083-3_16)] [[code](https://github.com/Hasindri/HLSS)]  

**[ECCV 2025] Knowledge-enhanced visual-language pretraining for computational pathology**[[paper](https://arxiv.org/pdf/2404.09942)] [[code](https://github.com/MAGIC-AI4Med/KEP)]  

**[LNCS 2024] Boosting Vision-Language Models for Histopathology Classification: Predict all at once**[[paper](https://arxiv.org/pdf/2409.01883)] [[code](https://github.com/FereshteShakeri/Histo-TransCLIP)]  

**[CVPR 2024] Generalizable whole slide image classification with fine-grained visual-semantic interaction**[[paper](https://openaccess.thecvf.com/content/CVPR2024/papers/Li_Generalizable_Whole_Slide_Image_Classification_with_Fine-Grained_Visual-Semantic_Interaction_CVPR_2024_paper.pdf)] [[code](https://github.com/ls1rius/WSI_FIVE)]  

**[CVPR 2024] ViLa-MIL: Dual-scale Vision-Language Multiple Instance Learning for Whole Slide Image Classification** [[paper](https://openaccess.thecvf.com/content/CVPR2024/papers/Shi_ViLa-MIL_Dual-scale_Vision-Language_Multiple_Instance_Learning_for_Whole_Slide_Image_CVPR_2024_paper.pdf)] [[code](https://github.com/Jiangbo-Shi/ViLa-MIL)]  

**[MICCAI 2024] Few-shot Adaptation of Medical Vision-Language Models**[[paper]](https://arxiv.org/pdf/2409.03868)[[code]](https://github.com/FereshteShakeri/few-shot-MedVLMs?tab=readme-ov-file)

**[CVPR 2025] No Pains, More Gains:Recycling Sub-Salient Patches for Efficient High-Resolution Image Recognition**[[paper]](https://openaccess.thecvf.com/content/CVPR2025/papers/Qin_No_Pains_More_Gains_Recycling_Sub-Salient_Patches_for_Efficient_High-Resolution_CVPR_2025_paper.pdf)[[code]](https://github.com/Qinrong-NKU/DBPS)

**[ 2025] THUNDER:Tile-level Histopathology image UNDERstanding benchmark**[[paper]](https://arxiv.org/pdf/2507.07860)[[code]](https://github.com/MICS-Lab/thunder)

**[2025] Uncertainty-Driven Expert Control: Enhancing the Reliability of Medical Vision-Language Models**[[paper]](https://www.arxiv.org/pdf/2507.09209)[[code]](https://github.com/ecoxial2007/Expert-CFG)

**[CVPR 2024] Transductive Zero-Shot and Few-Shot CLIP**[[paper]](https://openaccess.thecvf.com/content/CVPR2024/papers/Martin_Transductive_Zero-Shot_and_Few-Shot_CLIP_CVPR_2024_paper.pdf)[[code]](https://github.com/SegoleneMartin/transductive-CLIP)

**[IPMI 2025] A Reality Check of Vision-Language Pre-training in Radiology: Have We Progressed Using Text?**[[paper]](https://arxiv.org/pdf/2504.05227)[[code]](https://github.com/jusiro/DLILP)

**[MICCAI 2025] Few-Shot, Now for Real: Medical VLMs Adaptation without Balanced Sets or Validation**[[paper]](https://arxiv.org/pdf/2506.17500)[[code]](https://github.com/jusiro/SS-Text)

**[CVPR 2025] Fast and Accurate Gigapixel Pathological Image Classification with Hierarchical Distillation Multi-Instance Learning**[[paper](https://arxiv.org/pdf/2502.21130)] [[code](https://github.com/JiuyangDong/HDMIL)] 

**[CVPR 2025] MExD:AnExpert-InfusedDiffusionModelforWhole-SlideImageClassification**[[paper](https://arxiv.org/pdf/2503.12401)] [[code](https://github.com/JWZhao-uestc/MExD)]  

**[CVPR 2025] FOCUS: Knowledge-enhanced Adaptive Visual Compression for Few-shot Whole Slide Image Classification**[[paper](https://arxiv.org/pdf/2411.14743)] [[code](https://github.com/dddavid4real/FOCUS)]  

**[ICCV 2025] Rethinking Few-Shot Image Classification: a Good Embedding Is All You Need?**[[paper]](https://arxiv.org/pdf/2003.11539)[[code]](https://github.com/WangYueFt/rfs/)

**[ICLR 2025] Efficient and Context-Aware Label Propagation for Zero-/Few-Shot Training-Free Adaptation of Vision-Language Model**[[paper]](https://arxiv.org/pdf/2412.18303)[[code]](https://github.com/Yushu-Li/ECALP)

**[CVPR 2025] COSMIC:Clique-Oriented Semantic Multi-space Integration for Robust CLIP Test-Time Adaptation** [[paper](https://arxiv.org/pdf/2503.23388)] [[code](https://github.com/hf618/COSMIC)]  

**[CVPR 2025] Text Augmented Correlation Transformer For Few-shot Classification & Segmentation**[[paper]](https://openaccess.thecvf.com/content/CVPR2025/papers/Nandam_Text_Augmented_Correlation_Transformer_For_Few-shot_Classification__Segmentation_CVPR_2025_paper.pdf)

**[ICCV 2025] Think Twice: Test-Time Reasoning for Robust CLIP Zero-Shot Classification**[[paper]](https://iccv.thecvf.com/virtual/2025/poster/1492)

**[ICCV 2025] One Last Attention for your Vision-Language Model**[[paper]](https://arxiv.org/pdf/2507.15480)[[code]](https://github.com/khufia/RAda/tree/main)

**[MICCAI 2025] Conservative-Radical Complementary Learning for Class-incremental Medical Image Analysis with Pre-trained Foundation Models**[[paper]](https://papers.miccai.org/miccai-2025/paper/0019_paper.pdf)[[code]](https://github.com/CUHK-BMEAI/CRCL)

**[NeurIPS 2024]FAST: A Dual-tier Few-Shot Learning Paradigm for Whole Slide Image Classification**[[paper]](https://proceedings.neurips.cc/paper_files/paper/2024/file/bdcdf38389d7fcefc73c4c3720217155-Paper-Conference.pdf)[[code]](https://github.com/fukexue/FAST)

**[ICLR 2026] FLOW-BASED ALIGNMENT OF UNI-MODAL VISION AND TEXT ENCODERS FOR FEW-SHOT IMAGE CLASSIFICATION**[[paper]](https://openreview.net/attachment?id=educGk5ykl&name=pdf)[[code]]()

**[ICLR 2026]  EXPLOITING LOW-DIMENSIONAL MANIFOLD OF FEATURES FORFEW-SHOT WHOLE SLIDE IMAGE CLASSIFICATION**[[paper]](https://openreview.net/attachment?id=HBP9uSEYME&name=pdf)[[code]]()

**[MedAGI 2025] Test-Time Adaptation of Medical Vision-Language Models**[[paper]](https://link.springer.com/chapter/10.1007/978-3-032-07845-2_18)[[code]](https://github.com/FereshteShakeri/TTAMedVLMs?tab=readme-ov-file#test-time-adaptation-of-medical-vlms)

**[NeurIPS 2025]RadZero: Similarity-Based Cross-Attention for Explainable Vision-Language Alignment in Chest X-ray with Zero-Shot Multi-Task Capability**[[paper]](https://arxiv.org/pdf/2504.07416)[[code]]( https://github.com/deepnoid-ai/RadZero)

**[NeurIPS 2025]Few-Shot Learning from Gigapixel Images via Hierarchical Vision-Language Alignment and Modeling**[[paper]](https://arxiv.org/pdf/2505.17982)[[code]]( https://github.com/bryanwong17/HiVE-MIL)

**[NeurIPS 2025] MAPLE: Multi-scale Attribute-enhanced Prompt Learning for Few-shot Whole Slide Image Classification**[[paper]](https://arxiv.org/pdf/2509.25863)[[code]](https://github.com/JJ-ZHOU-Code/MAPLE)

**[NeurIPS 2025] Conditional Representation Learning for Customized Tasks**[[paper]](https://arxiv.org/pdf/2510.04564)[[code]](https://github.com/XLearning-SCU/2025-NeurIPS-CRL)

**[CVPR 2025] Towards All-in-One Medical Image Re-Identification**[[paper]](https://openaccess.thecvf.com/content/CVPR2025/papers/Tian_Towards_All-in-One_Medical_Image_Re-Identification_CVPR_2025_paper.pdf) [[code]](https://github.com/tianyuan168326/All-in-One-MedReID-Pytorch) 

**[MICCAI 2025] Background-Invariant Independence-Guided Multi-head Attention Network for Skin Lesion Classification** [[paper](https://papers.miccai.org/miccai-2025/paper/2868_paper.pdf)] [[code]](https://github.com/shb2908/BIIGMA-Net) 

**[MICCAI 2025] Knowledge-guided Multi-scale Graph Mamba for Whole Slide Image Classification** [[paper]](https://papers.miccai.org/miccai-2025/paper/0569_paper.pdf) [[code]]() 

**[MICCAI 2025] Medical Contrastive Learning of Positive and Negative Mentions** [[paper]](https://papers.miccai.org/miccai-2025/paper/3892_paper.pdf) [[code]](https://github.com/WVeLong/VECL) 

**[MICCAI 2025] MOTOR: Multimodal Optimal Transport via Grounded Retrieval in Medical Visual Question Answering** [[paper]](https://papers.miccai.org/miccai-2025/paper/2665_paper.pdf) [[code]](https://github.com/BioMedIA-MBZUAI/MOTOR) 

**[MICCAI 2025] OpenPath: Open-Set Active Learning for Pathology Image Classification via Pre-trained Vision-Language Models** [[paper]](https://papers.miccai.org/miccai-2025/paper/1566_paper.pdf) [[code]](https://github.com/HiLab-git/OpenPath) 

**[MICCAI 2025] Temporal Model-Based Federated Active Medical Image Classification** [[paper]](https://papers.miccai.org/miccai-2025/paper/1626_paper.pdf) [[code]](https://github.com/IAMJackYan/TM-FAL) 

**[MICCAI 2025] Training-free Test-time Improvement for Explainable Medical Image Classification** [[paper]](https://papers.miccai.org/miccai-2025/paper/1672_paper.pdf) [[code]](https://github.com/riverback/TF-TTI-XMed) 

**[MICCAI 2025] Weighted Stratification in Multi-Label Contrastive Learning for Long-Tailed Medical Image Classification** [[paper]](https://papers.miccai.org/miccai-2025/paper/1834_paper.pdf) [[code]](https://github.com/xup6YJ/ws-MulSupCon) 

**[MICCAI 2025] XFMamba: Cross-Fusion Mamba for Multi-View Medical Image Classification** [[paper]](https://papers.miccai.org/miccai-2025/paper/1773_paper.pdf) [[code]](https://github.com/XZheng0427/XFMamba) 

**[MICCAI 2025] Your other Left! Vision-Language Models Fail to Identify Relative Positions in Medical Images** [[paper]](https://papers.miccai.org/miccai-2025/paper/0530_paper.pdf) [[code]](https://wolfda95.github.io/your_other_left/) 

**[MICCAI 2025] BiMSRec: A Progressive Image Reconstruction Framework for Medical Image Fusion Guided by Multi-Scale Deformation Fields** [[paper]](https://papers.miccai.org/miccai-2025/paper/1799_paper.pdf) [[code]]() 

**[MICCAI 2025] BayeSMM: Robust Deep Combined Computing Tackling Heavy-tailed Distribution in Medical Images** [[paper]](https://papers.miccai.org/miccai-2025/paper/0645_paper.pdf) [[code]](https://github.com/HenryLau7/BayeSMM) 

**[MICCAI 2025] AEM: Attention Entropy Maximization for Multiple Instance Learning based Whole Slide Image Classification** [[paper]](https://papers.miccai.org/miccai-2025/paper/5183_paper.pdf) [[code]](https://github.com/dazhangyu123/AEM) 

### 生存分析
**[Nature 2024] A pathology foundation model for cancer diagnosis and prognosis prediction** [[paper]](https://www.nature.com/articles/s41586-024-07894-z) [[code]]() 

**[CVPR 2024] Tumor Micro-environment Interactions Guided Graph Learning for Survival Analysis of Human Cancers from Whole-slide Pathological Images**[[paper]](https://openaccess.thecvf.com/content/CVPR2024/papers/Shao_Tumor_Micro-environment_Interactions_Guided_Graph_Learning_for_Survival_Analysis_of_CVPR_2024_paper.pdf) [[code]]() 

**[CVPR 2024] Modeling Dense Multimodal Interactions Between Biological Pathways and Histology for Survival Prediction**[[paper]](https://openaccess.thecvf.com/content/CVPR2024/papers/Jaume_Modeling_Dense_Multimodal_Interactions_Between_Biological_Pathways_and_Histology_for_CVPR_2024_paper.pdf) [[code]](https://github.com/mahmoodlab/SurvPath) 

**[AAAI 2025] ConSurv: Multimodal Continual Learning for Survival Analysis** [[paper]](https://arxiv.org/pdf/2511.09853) [[code]](https://github.com/LucyDYu/ConSurv) 


**[ICCV 2025] PS3: A Multimodal Transformer Integrating Pathology Reports with Histology Images and Biological Pathways for Cancer Survival Prediction**[[paper]](https://openaccess.thecvf.com/content/ICCV2025/papers/Raza_PS3_A_Multimodal_Transformer_Integrating_Pathology_Reports_with_Histology_Images_ICCV_2025_paper.pdf?utm_source=chatgpt.com)[[code]]()

**[Nature 2025] A foundation model for generalizable cancer diagnosis and survival prediction from histopathological images** [[paper]](https://www.nature.com/articles/s41467-025-57587-y?utm_source=chatgpt.com) [[code]]() 

**[NeurIPS 2025] Cancer Survival Analysis via Zero-shot Tumor Microenvironment Segmentation on Low-resolution Whole Slide Pathology Images** [[paper]](https://openreview.net/pdf?id=BkSRQ1y37l) [[code]]() 

**[ICCV 2025] Graph Domain Adaptation with Dual-branch Encoder and Two-level Alignment for Whole Slide Image-based Survival Prediction**[[paper]](https://arxiv.org/pdf/2411.14001)[[code]](https://github.com/yuntaoshou/DETA)

**[ICCV 2025] PS3: A Multimodal Transformer Integrating Pathology Reports with Histology Images and Biological Pathways for Cancer Survival Prediction**[[paper]](https://openaccess.thecvf.com/content/ICCV2025/papers/Raza_PS3_A_Multimodal_Transformer_Integrating_Pathology_Reports_with_Histology_Images_ICCV_2025_paper.pdf?utm_source=chatgpt.com)[[code]]()



**[Bioinformatics 2025] Deep learning-driven survival prediction in pan-cancer studies by integrating multimodal histology-genomic data** [[paper]](https://watermark02.silverchair.com/bbaf121.pdf?token=AQECAHi208BE49Ooan9kkhW_Ercy7Dm3ZL_9Cf3qfKAc485ysgAAA1AwggNMBgkqhkiG9w0BBwagggM9MIIDOQIBADCCAzIGCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQMlH4-IggTL5hXJB8aAgEQgIIDAysc-Kp7c0hMqATVcoGRNDRB7nnvo_aPJvNSf8vvO3fSapqVVOc4wDYsebGLeTeh22S03iETsVgTuxxbgOiPe4q7ct8UaTzAQsGyZI0x-ilqhJJ_pF4qEZ9zOLDu0tIFWM7TMEBadWhiIi-5XeSu2iRflc15n9ppK4Qhhg8auldY7DZbcNzLX1nKO1bds0anramsU5Fu9LsTNRJulWLFaZrk1KP9UsbcINFpsRPit2n0dxCcLKOE0P95EOBnQjimAwYFroz8dG9qsEGX82ffp25i4wPKLDT090A5TGs1smaY2KhvX7jOJkCFC-cT_cbpkI3I4Qgybk9lR8xvt_51Bbdn9uivP8wfVRIPHMndlsVgbSSBxTcAGOzAMEHbKfMRovqVvjUlK1bqVjmU47Bu1zOfo8FXbF0VvvMgMK-h8n61BrdYwhS9P-gsz8-y2rINEgzGGf0MhyfSYCLKKmRHH_ubU9wdy6VXyFaLUeGNoP0-WshbSvjkYsxHC98n8R62euumh7Smr5m3esyhBQAHp-W0GFZPZVPE9-BWmxhlSOEbxLW7i7pP5K8mZXiGSiy7ebQS4qIjD_zZ_bxLbQoQwjsdW75eYfexZSUQ_ZifPY-yrF9mJdwY-mKTe5mhBrulxf5M6eOG70OSBcs7tzZUAoj-A0bXps2HNn-LkE1Dh2pXAW9hBjzrS7YxmIeX__83YJN4Aq-PyXtt1XKCRFbmwKOmdjy1-trqzW3xQ10jYgU3Jldale3eehOAqpWkoOvNWEy4XoCXH_4TawGNH4F3ivfvdlrldul00A3Sl3LRUo0s1AJLVWg2dAv9GIPiWYBfGupdLyJKEPobWzyzZMmY3_iq63lpm31hxerx-t_0CfS8vek91FXdrCxsVOUs_nDhI9-tdDbXGlNunmLAS2Ol-Uw5fGhpmQFT4N4pjzCJ7S72JbQfqiQ2PZXQHWJw021e4xlcdWvoMukOMszQkunfMk0bsCe-X-TfggTRmPZPbT5FB3l0MILuQAhvLMIOYT0UA8FBcQ) [[code]]() 

**[MICCAI 2025] Multimodal Cross-Task Interaction for Survival Analysis in Whole Slide Pathological Images** [[paper]](https://papers.miccai.org/miccai-2024/paper/1280_paper.pdf?utm_source=chatgpt.com) [[code]](https://github.com/jsh0792/MCTI) 

**[MICCAI 2025] Survival analysis of histopathological image based on a pretrained hypergraph model of spatial transcriptomics data** [[paper]](https://papers.miccai.org/miccai-2024/paper/4136_paper.pdf?utm_source=chatgpt.com) [[code]]() 

**[MICCAI 2025] LLM-guided Multi-modal Multiple Instance Learning for 5-year Overall Survival Prediction of Lung Cancer** [[paper]](https://papers.miccai.org/miccai-2024/paper/2173_paper.pdf) [[code]](https://github.com/KyleKWKim/LLM-guided-Multimodal-MIL) 

**[NeurIPS 2025] Leveraging Tumor Heterogeneity: Heterogeneous Graph Representation Learning for Cancer Survival Prediction in Whole Slide Images** [[paper]](https://proceedings.neurips.cc/paper_files/paper/2024/file/760341adc5632de3f1cf2e8d22215a93-Paper-Conference.pdf) [[code]](https://github.com/wjx-error/ProtoSurv) 

**[MICCAI 2025] A flexible deep learning framework for survival analysis with medical data** [[paper]](https://papers.miccai.org/miccai-2025/paper/1381_paper.pdf) [[code]]() 

**[MICCAI 2025] Enhancing WSI-Based Survival Analysis with Report-Auxiliary Self-Distillation** [[paper]](https://papers.miccai.org/miccai-2025/paper/1264_paper.pdf?utm_source=chatgpt.com) [[code]](https://github.com/zhengwang9/Rasa) 

**[MICCAI 2025] Cross-Modality Masked Learning for Survival Prediction in ICI Treated NSCLC Patients** [[paper]](https://papers.miccai.org/miccai-2025/paper/0301_paper.pdf?utm_source=chatgpt.com) [[code]]() 

**[MICCAI 2025] SCMIL: Sparse Context-aware Multiple Instance Learning for Predicting Cancer Survival Probability Distribution in Whole Slide Images** [[paper]](https://papers.miccai.org/miccai-2024/paper/2991_paper.pdf?utm_source=chatgpt.com) [[code]](https://github.com/yang-ze-kang/SCMIL) 











### 病理图像检测
**[CVPR 2024] Adapting Visual-Language Models for Generalizable Anomaly Detection in Medical Images**
[[Paper]](https://openaccess.thecvf.com/content/CVPR2024/papers/Huang_Adapting_Visual-Language_Models_for_Generalizable_Anomaly_Detection_in_Medical_Images_CVPR_2024_paper.pdf)
[[Code]](https://github.com/MediaBrain-SJTU/MVFA-AD)

**[ICCV 2025] Unknown Text Learning for CLIP-based Few-Shot Open-set Recognition**[[paper]](https://iccv.thecvf.com/virtual/2025/poster/2460)

### Foundation Models & Pre-training
**[2023-Arxiv] Foundation Models in Computational Pathology: A Review of Challenges, Opportunities, and Impact**[[paper](https://arxiv.org/pdf/2502.08333)]  

**[CVPR 2025] CPath-Omni: A Unified Multimodal Foundation Model for Patch and Whole Slide Image Analysis in Computational Pathology**[[paper](https://arxiv.org/pdf/2412.12077)]  

**[CVPR 2025] ulti-Resolution Pathology-Language Pre-training Model with Text-Guided Visual Representation**[[paper](https://openaccess.thecvf.com/content/CVPR2025/papers/Albastaki_Multi-Resolution_Pathology-Language_Pre-training_Model_with_Text-Guided_Visual_Representation_CVPR_2025_paper.pdf)] [[code](https://github.com/BasitAlawode/MR-PLIP)]  

**[Nature 2025] BEPH:A foundation model for generalizable cancer diagnosis and survival prediction from histopathological images**[[paper](https://www.nature.com/articles/s41467-025-57587-y#Sec10)] [[code](https://github.com/Zhcyoung/BEPH)]  

**[Nature 2025] A vision–language foundation model for precision oncology**[[paper](https://www.nature.com/articles/s41586-024-08378-w#code-availability)]  

**[CVPR 2025] Realistic Test-Time Adaptation of Vision-Language Models** [[paper](https://openaccess.thecvf.com/content/CVPR2025/papers/Zanella_Realistic_Test-Time_Adaptation_of_Vision-Language_Models_CVPR_2025_paper.pdf)] [[code](https://github.com/MaxZanella/StatA)]  

**[2025-Arxiv] GECKO:GigapixelVision-Concept Contrastive Pretraining in Histopathology** [[paper](https://arxiv.org/pdf/2504.01009)] [[code](https://github.com/bmi-imaginelab/GECKO)]  

**[2025-Arxiv] Multi-Modal Foundation Models for Computational Pathology:A Survey**[[paper](https://arxiv.org/pdf/2503.09091)]  

**[2025-Arxiv] Unsupervised Foundation Model-Agnostic Slide-Level Representation Learning** [[paper](https://arxiv.org/pdf/2411.13623)] [[code](https://github.com/KatherLab/COBRA)] 

**[2025-Arxiv] Full Conformal Adaptation of Medical Vision-Language Models** [[paper](https://arxiv.org/pdf/2506.06076)] [[code](https://github.com/jusiro/FCA)]  

**[CVPR 2025] GET: Unlocking the Multi-modal Potential of CLIP for Generalized Category Discovery**[[paper](https://arxiv.org/pdf/2403.09974)] [[code](https://github.com/enguangW/GET)]  

**[ICCV 2025] One Last Attention for your Vision-Language Model**[[paper]](https://github.com/khufia/RAda/tree/main)[[code]](https://github.com/khufia/RAda/tree/main)

**[ICCV 2025] CLIP-GS: Unifying Vision-Language Representation with 3D Gaussian Splatting**[[paper]](https://arxiv.org/pdf/2412.19142)[[code]]()

**[ICCV 2025] Multi-Cache Enhanced Prototype Learning for Test-Time Generalization of Vision-Language Models**[[paper]](https://iccv.thecvf.com/virtual/2025/poster/1177)

**[ICCV 2025] Test-Time Retrieval-Augmented Adaptation for Vision-Language Models**[[paper]](https://iccv.thecvf.com/virtual/2025/poster/2327)


### Few-shot合集
**[ICCV 2023] Black Box Few-Shot Adaptation for Vision-Language models**[[paper]](https://openaccess.thecvf.com/content/ICCV2023/papers/Ouali_Black_Box_Few-Shot_Adaptation_for_Vision-Language_Models_ICCV_2023_paper.pdf)[[code]](https://github.com/saic-fi/LFA)

**[2024]Selective Vision-Language Subspace Projection for Few-shot CLIP**[[paper]](https://arxiv.org/pdf/2407.16977?)[[code]](https://github.com/zhuhsingyuu/SSP)

**[ICCV 2025] CorrCLIP: Reconstructing Patch Correlations in CLIP for Open-Vocabulary Semantic Segmentation**[[paper]](https://arxiv.org/pdf/2411.10086)[[code]](https://github.com/zdk258/CorrCLIP)


## 综述
**[2025] Adapting Vision-Language Models Without Labels:A Comprehensive Survey**[[paper]](https://arxiv.org/pdf/2508.05547?)[[code]](https://github.com/tim-learn/Awesome-LabelFree-VLMs)


## KNN & retrieval

**1、[NeurIPS 2021]Re-ranking for image retrieval and transductive few-shot classification**[[paper]](https://proceedings.neurips.cc/paper_files/paper/2021/file/d9fc0cdb67638d50f411432d0d41d0ba-Paper.pdf)[[code]](https://imagine.enpc.fr/~shenx/SSR/)

**2、[CVPR 2017]Re-ranking Person Re-identification with k-reciprocal Encoding**[[paper]](https://arxiv.org/pdf/1701.08398)[[code]](https://github.com/zhunzhong07/person-re-ranking)

**3、[CVPR 2025]Cheb-GR: Rethinking k-nearest neighbor search in Re-ranking for Person Re-identification**[[paper]](https://openaccess.thecvf.com/content/CVPR2025/papers/Yang_Cheb-GR_Rethinking_K-nearest_Neighbor_Search_in_Re-ranking_for_Person_Re-identification_CVPR_2025_paper.pdf)[[code]](https://github.com/Jinxi-Yang-WHU/Fast-GCR)






## 自然图像
### Weakly Supervised Semantic Segmentation

 **1、Learning Multi-Modal Class-Specific Tokens for Weakly Supervised Dense Object Localization**[[paper]](https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Learning_Multi-Modal_Class-Specific_Tokens_for_Weakly_Supervised_Dense_Object_Localization_CVPR_2023_paper.pdf)[[code]](https://github.com/xulianuwa/MMCST)

**2、[CVPR 2022]Learning Affinity from Attention: End-to-End Weakly-Supervised Semantic Segmentation with Transformers**[[paper]](https://openaccess.thecvf.com/content/CVPR2022/papers/Ru_Learning_Affinity_From_Attention_End-to-End_Weakly-Supervised_Semantic_Segmentation_With_Transformers_CVPR_2022_paper.pdf)[[code]](https://github.com/rulixiang/afa)

**3、[CVPR 2022]MCTFormer:Multi-Class Token Transformer for Weakly Supervised Semantic Segmentation**[[paper]](https://arxiv.org/pdf/2203.02891)[[code]](https://github.com/xulianuwa/MCTformer)

**4、[CVPR 2022] MCTformer+: Multi-Class Token Transformer for Weakly Supervised Semantic Segmentation**[[paper]](https://arxiv.org/pdf/2308.03005)[[code]](https://github.com/xulianuwa/MCTformer)

**5、[ICCV 2023]Spatial-Aware Token for Weakly Supervised Object Localization** [[paper]](https://openaccess.thecvf.com/content/ICCV2023/papers/Wu_Spatial-Aware_Token_for_Weakly_Supervised_Object_Localization_ICCV_2023_paper.pdf) [[code](https://github.com/wpy1999/SAT))

**6、[CVPR 2023]Boundary-enhanced Co-training for Weakly Supervised Semantic Segmentation**[[paper]](https://openaccess.thecvf.com/content/CVPR2023/papers/Rong_Boundary-Enhanced_Co-Training_for_Weakly_Supervised_Semantic_Segmentation_CVPR_2023_paper.pdf)[[code]](https://github.com/ShenghaiRong/BECO?tab=readme-ov-file)

**7、[ICCV 2023]Zero-Shot Composed Image Retrieval with Textual Inversion**[[paper]](https://openaccess.thecvf.com/content/ICCV2023/papers/Baldrati_Zero-Shot_Composed_Image_Retrieval_with_Textual_Inversion_ICCV_2023_paper.pdf)[[code]](https://github.com/miccunifi/SEARLE.)

**8、[CVPR 2023]Learning Multi-Modal Class-Specific Tokens for Weakly Supervised Dense Object Localization** [[paper]](https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Learning_Multi-Modal_Class-Specific_Tokens_for_Weakly_Supervised_Dense_Object_Localization_CVPR_2023_paper.pdf) [[code]](https://github.com/xulianuwa/MMCST)\

**9、[CVPR 2023]ToCo:Token Contrast for Weakly-Supervised Semantic Segmentation**[[paper]](https://arxiv.org/pdf/2303.01267)[[code]](https://github.com/rulixiang/ToCo)

**10、[CVPR 2024]Frozen CLIP: A Strong Backbone for Weakly Supervised Semantic Segmentation** [[paper]](https://arxiv.org/pdf/2406.11189v1) [[code]](https://github.com/zbf1991/WeCLIP)

**11、[CVPR 2024]DuPL: Dual Student with Trustworthy Progressive Learning for RobustWeakly Supervised Semantic Segmentation** [[paper]](https://arxiv.org/pdf/2403.11184)[[code]](https://github.com/Wu0409/DuPL)

**12、[CVPR 2024]Hunting Attributes: Context Prototype-Aware Learning for Weakly Supervised Semantic Segmentation** [[paper]](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2403.07630)[[code]](https://github.com/Barrett-python/CPAL))

**13、[ECCV 2024]DIAL: Dense Image-text ALignment for Weakly Supervised Semantic Segmentation**[[paper]](https://arxiv.org/pdf/2409.15801)

**14、[CVPR 2024]Separate and Conquer: Decoupling Co-occurrence via Decomposition and Representation for Weakly Supervised Semantic Segmentation**[[paper]](https://arxiv.org/pdf/2402.18467)[[code]](https://github.com/zwyang6/SeCo)

**15、[ECCV 2024]CoSa:Weakly Supervised Co-training with Swapping Assignments for Semantic Segmentation**[[paper]](https://arxiv.org/pdf/2402.17891)[[code]](https://github.com/youshyee/CoSA)

**16、[IEEE 2024]SSC:Spatial Structure Constraints for Weakly Supervised Semantic Segmentation**[[paper]](https://arxiv.org/pdf/2401.11122)[[code](https://github.com/NUST-Machine-Intelligence-Laboratory/SSC)

**17、[AAAI 2024]Progressive Feature Self-Reinforcement for Weakly Supervised Semantic Segmentation**[[paper]](https://arxiv.org/pdf/2312.08916)[[code]](https://github.com/Jessie459/feature-self-reinforcement)

**18、[CVPR 2024]Class Tokens Infusion for Weakly Supervised Semantic Segmentation**[[paper]](https://openaccess.thecvf.com/content/CVPR2024/papers/Yoon_Class_Tokens_Infusion_for_Weakly_Supervised_Semantic_Segmentation_CVPR_2024_paper.pdf)[[code]]( https://github.com/yoon307/CTI)

**19、[CVPR 2024]SFC: Shared Feature Calibration in Weakly Supervised Semantic Segmentation**[[paper]](https://arxiv.org/pdf/2401.11719)[[code]](https://github.com/Barrett-python/SFC)

**20、[CVPR 2024]PSDPM:Prototype-based Secondary Discriminative Pixels Mining for Weakly Supervised Semantic Segmentation**[[paper]](https://openaccess.thecvf.com/content/CVPR2024/papers/Zhao_PSDPM_Prototype-based_Secondary_Discriminative_Pixels_Mining_for_Weakly_Supervised_Semantic_CVPR_2024_paper.pdf)[[code]](https://github.com/xinqiaozhao/PSDPM)

**21、[AAAI 2025]MoRe: Class Patch Attention Needs Regularization for Weakly Supervised Semantic Segmentation**[[paper]](https://arxiv.org/pdf/2412.11076)[[code]](https://github.com/zwyang6/MoRe)

**22、[CVPR 2025]PROMPT-CAM: A Simpler Interpretable Transformer for Fine-Grained Analysis**[[paper]](https://arxiv.org/pdf/2501.09333)[[code]](https://github.com/Imageomics/Prompt_CAM)

**23、[CVPR 2025]Exploring CLIP’s Dense Knowledge for Weakly、 Supervised Semantic Segmentation**[[paper]](https://arxiv.org/pdf/2503.20826)[[code]](https://github.com/zwyang6/ExCEL)

**24、[CVPR 2025]GET: Unlocking the Multi-modal Potential of CLIP for Generalized Category Discovery**[[paper]](https://arxiv.org/abs/2403.09974)[[code]](https://github.com/enguangW/GET)

**25、[2025]TeD-Loc: Text Distillation for Weakly Supervised Object Localization**[[paper]](https://arxiv.org/pdf/2501.12632)[[code]](https://github.com/shakeebmurtaza/TeDLOC)

**26、[2025]Image Augmentation Agent for Weakly Supervised Semantic Segmentation**[[paper]](https://arxiv.org/pdf/2412.20439)

**27、[CVPR 2025]Multi-Label Prototype Visual Spatial Search for Weakly Supervised Semantic Segmentation**

### 分类
**[ICCV 2025] Generate, Transduct, Adapt: Iterative Transduction with VLMs**[[paper]](https://arxiv.org/pdf/2501.06031)[[code]](https://github.com/cvl-umass/GTA-CLIP))


### Weakly Supervised Object Localization
**1、[2024]A Realistic Protocol for Evaluation of Weakly Supervised Object Localization**[[paper](https://arxiv.org/pdf/2404.10034)[[code]](https://github.com/shakeebmurtaza/wsol_model_selection)

## 聚类
**1、[ICML 2024]Image Clustering with External Guidance**[[paper]](https://arxiv.org/pdf/2310.11989)[[code]](https://github.com/XLearning-SCU/2024-ICML-TAC)

**2、[TPAMI 2024]Simplex Clustering via sBeta With Applications to Online Adjustment of Black-Box Predictions**[[paper]](https://arxiv.org/pdf/2208.00287)[[code]](https://github.com/fchiaroni/Clustering_Softmax_Predictions)

### Multi-visual clustering
**1、[CVPR 2025]EASEMVC:Efficient Dual Selection Mechanism for Deep Multi-View Clustering**[[paper]](https://openaccess.thecvf.com/content/CVPR2025/papers/Xiao_EASEMVCEfficient_Dual_Selection_Mechanism_for_Deep_Multi-View_Clustering_CVPR_2025_paper.pdf)

**2、[CVPR 2025]Enhanced then Progressive Fusion with View Graph for Multi-View Clustering**[[paper]](https://openaccess.thecvf.com/content/CVPR2025/papers/Dong_Enhanced_then_Progressive_Fusion_with_View_Graph_for_Multi-View_Clustering_CVPR_2025_paper.pdf)

**3、[CVPR 2025]Imputation-free and Alignment-free: Incomplete Multi-view Clustering Driven by Consensus Semantic Learning**[[paper]](https://openaccess.thecvf.com/content/CVPR2025/papers/Dai_Imputation-free_and_Alignment-free_Incomplete_Multi-view_Clustering_Driven_by_Consensus_Semantic_CVPR_2025_paper.pdf)

**4、[CVPR 2025]Deep Fair Multi-View Clustering with Attention KAN**[[paper]](https://openaccess.thecvf.com/content/CVPR2025/papers/Xu_Deep_Fair_Multi-View_Clustering_with_Attention_KAN_CVPR_2025_paper.pdf)

**5、[CVPR 2025]Medusa: A Multi-Scale High-order Contrastive Dual-Diffusion Approach for Multi-View Clustering**[[paper]](https://openaccess.thecvf.com/content/CVPR2025/papers/Chen_Medusa_A_Multi-Scale_High-order_Contrastive_Dual-Diffusion_Approach_for_Multi-View_Clustering_CVPR_2025_paper.pdf)

**6、[ICCV 2025]Generalized Deep Multi-view Clustering via Causal Learning with Partially Aligned Cross-view Correspondence**[[paper]](https://openaccess.thecvf.com/content/ICCV2025/papers/Yang_Generalized_Deep_Multi-view_Clustering_via_Causal_Learning_with_Partially_Aligned_ICCV_2025_paper.pdf?utm_source=chatgpt.com)

**7、[NeurIPS 2025]HSACC: Hybrid Semantic-Aligned Contrastive Clustering for Incomplete Multi-View Clustering**[[paper]](https://neurips.cc/virtual/2025/loc/san-diego/poster/118577)[[code]](https://github.com/XiaojianDing/2025-NeurIPS-HSACC/blob/main/README.md)

**8、[NeurIPS 2025]Learning from Disjoint Views: A Contrastive Prototype Matching Network for Fully Incomplete Multi-View Clustering**[[paper]](https://openreview.net/pdf?id=8wWHCzsTxS)[[code]](https://nips.cc/public/guides/CodeSubmissionPolicy)

**9、[NeurIPS 2025]Scalable Cross-View Sample Alignment for Multi-View Clustering with View Structure Similarity**[[paper]](https://openreview.net/pdf?id=oysfr9yqUI)[[code]](https://nips.cc/public/guides/CodeSubmissionPolicy)

**10、[NeurIPS 2025]Scalable Cross-View Sample Alignment for Multi-View Clustering with View Structure Similarity**[[paper]](https://openreview.net/pdf?id=oysfr9yqUI)[[code]](https://nips.cc/public/guides/CodeSubmissionPolicy)

**11、[NeurIPS 2025]AF-UMC:An Alignment-Free Fusion Framework for Unaligned Multi-View Clustering**[[paper]](https://openreview.net/pdf?id=G1jrjumK1b)[[code]](https://nips.cc/public/guides/CodeSubmissionPolicy)

**12、[ICML 2025]Multi-View Graph Clustering via Node-Guided Contrastive Learning and Adaptive Fusion**[[paper]](https://openreview.net/pdf?id=Ae5qnQxAxQ)[[code]]()

**13、[ICLR 2025]SIMPLE YET EFFECTIVE INCOMPLETE MULTI-VIEW CLUSTERING: SIMILARITY-LEVEL IMPUTATION AND INTRA-VIEW HYBRID-GROUP PROTOTYPE CONSTRUCTION**[[paper]](https://openreview.net/pdf?id=KijslFbfOL)[[code]]()

## BackBone
**1、[CVPR 2016] CAM:Learning Deep Features for Discriminative Localization**
[[paper]](https://arxiv.org/pdf/1512.04150)

**2、[ICCV 2021] Swin Transformer: Hierarchical Vision Transformer using Shifted Windows**
[[paper]](https://arxiv.org/pdf/2103.14030)[[code]](https://github.com/microsoft/Swin-Transformer)

## benchmark
https://github.com/FereshteShakeri/few-shot-MedVLMs

**1、[NeurIPS25 D&B Spotlight] A tile-level histopathology image understanding benchmark**[[paper]](https://arxiv.org/pdf/2507.07860)[[code]](https://github.com/MICS-Lab/thunder)

**2、[nature biomedical engineering]Benchmarking foundation models as feature extractors for weakly-supervised computational pathology**[[paper]](https://arxiv.org/pdf/2408.15823)[[code]]()




<a name="Dataset"></a>
# Dataset:
- [x] [Cityscapes](https://github.com/open-mmlab/mmsegmentation/blob/master/docs/en/dataset_prepare.md#cityscapes)
- [x] [PASCAL VOC](https://github.com/open-mmlab/mmsegmentation/blob/master/docs/en/dataset_prepare.md#pascal-voc)
- [x] [ADE20K](https://github.com/open-mmlab/mmsegmentation/blob/master/docs/en/dataset_prepare.md#ade20k)
- [x] [Pascal Context](https://github.com/open-mmlab/mmsegmentation/blob/master/docs/en/dataset_prepare.md#pascal-context)
- [x] [COCO-Stuff 10k](https://github.com/open-mmlab/mmsegmentation/blob/master/docs/en/dataset_prepare.md#coco-stuff-10k)
- [x] [COCO-Stuff 164k](https://github.com/open-mmlab/mmsegmentation/blob/master/docs/en/dataset_prepare.md#coco-stuff-164k)
- [x] [CHASE_DB1](https://github.com/open-mmlab/mmsegmentation/blob/master/docs/en/dataset_prepare.md#chase-db1)
- [x] [DRIVE](https://github.com/open-mmlab/mmsegmentation/blob/master/docs/en/dataset_prepare.md#drive)
- [x] [HRF](https://github.com/open-mmlab/mmsegmentation/blob/master/docs/en/dataset_prepare.md#hrf)
- [x] [STARE](https://github.com/open-mmlab/mmsegmentation/blob/master/docs/en/dataset_prepare.md#stare)
- [x] [Dark Zurich](https://github.com/open-mmlab/mmsegmentation/blob/master/docs/en/dataset_prepare.md#dark-zurich)
- [x] [Nighttime Driving](https://github.com/open-mmlab/mmsegmentation/blob/master/docs/en/dataset_prepare.md#nighttime-driving)
- [x] [LoveDA](https://github.com/open-mmlab/mmsegmentation/blob/master/docs/en/dataset_prepare.md#loveda)
- [x] [Potsdam](https://github.com/open-mmlab/mmsegmentation/blob/master/docs/en/dataset_prepare.md#isprs-potsdam)
- [x] [Vaihingen](https://github.com/open-mmlab/mmsegmentation/blob/master/docs/en/dataset_prepare.md#isprs-vaihingen)
- [x] [iSAID](https://github.com/open-mmlab/mmsegmentation/blob/master/docs/en/dataset_prepare.md#isaid)
- [x] [High quality synthetic face occlusion](https://github.com/open-mmlab/mmsegmentation/blob/master/docs/en/dataset_prepare.md#delving-into-high-quality-synthetic-face-occlusion-segmentation-datasets)
- [x] [ImageNetS](https://github.com/open-mmlab/mmsegmentation/blob/master/docs/en/dataset_prepare.md#imagenets)

